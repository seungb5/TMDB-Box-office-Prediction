{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mktGBrXc7HHV"
   },
   "outputs": [],
   "source": [
    "# package\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "%matplotlib notebook\n",
    "from scipy.stats import skew, boxcox\n",
    "# import plotly.offline as py\n",
    "# py.init_notebook_mode(connected=True)\n",
    "# import plotly.graph_objs as go\n",
    "# import plotly.tools as tls\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import ast\n",
    "import re\n",
    "# import yaml\n",
    "import json\n",
    "from collections import Counter\n",
    "# from nltk.corpus import stopwords\n",
    "# from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "# import xgboost as xgb\n",
    "# import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# import eli5\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZlpW_2b7HHa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# method\n",
    "def date_features(df):\n",
    "    df[['release_month','release_day','release_year']]=df['release_date'].str.split('/',expand=True).replace(np.nan, -1).astype(int)\n",
    "    # 연도 끝 두자리수만 있기 때문에 앞에 19/20 붙이기\n",
    "    df.loc[ (train['release_year'] <= 19) & (df['release_year'] < 100), \"release_year\"] += 2000\n",
    "    df.loc[ (train['release_year'] > 19) & (df['release_year'] < 100), \"release_year\"] += 1900\n",
    "    return df\n",
    "\n",
    "def text_to_dict(df):\n",
    "    for column in dict_columns:\n",
    "        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D0CMTDiH7HHc"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6fa1ae58cff5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"..input/test.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0msubmission\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubmission_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "submission_path = \"..input/sample_submission.csv\"\n",
    "train_path = \"..input/train.csv\"\n",
    "test_path = \"..input/test.csv\"\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "submission = pd.read_csv(submission_path)\n",
    "test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EdCC5TFM7HH0"
   },
   "source": [
    "# 2.1 **Data Pre-processing & FE** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DEa3y-Vk7HH1"
   },
   "source": [
    "## 2.1.1. **date_features 변환**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OVmwQuBD7HH2"
   },
   "outputs": [],
   "source": [
    "train = date_features(train)\n",
    "test = date_features(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SPxOTdX37HH4"
   },
   "source": [
    "___\n",
    "## 2.1.2. **범주형 변수 변환**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cf7c7fc185dee82029257e3ac81d3f2293a5ca7b",
    "colab_type": "text",
    "id": "AREZG5es7HH5"
   },
   "source": [
    "## **Json Format Columns to Dictionary Format**\n",
    "<br>\n",
    "\n",
    "- Json 형식의 변수들을 사용하기 위해 dictionary 형식으로 변환하여 분석에 이용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "a2c066bad59d6370eaa3879ae1e608cba19854f9",
    "colab": {},
    "colab_type": "code",
    "id": "cRRrDIUJ7HH6"
   },
   "outputs": [],
   "source": [
    "dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n",
    "                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n",
    "\n",
    "train = text_to_dict(train)\n",
    "test = text_to_dict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "99c57c3877aeaa1eaec4804fecb3a7590fdf004a",
    "colab_type": "text",
    "id": "PpoxNAxA7HH_"
   },
   "source": [
    "## **Belong_to collection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "0c2e1999c4f9b2347a740ba24b625da89fbd3b72",
    "colab": {},
    "colab_type": "code",
    "id": "nahjpl7C7HID"
   },
   "outputs": [],
   "source": [
    "train['collection_name'] = train['belongs_to_collection'].apply(lambda x: x[0]['name'] if x != {} else 0)\n",
    "train['has_collection'] = train['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0)\n",
    "\n",
    "test['collection_name'] = test['belongs_to_collection'].apply(lambda x: x[0]['name'] if x != {} else 0)\n",
    "test['has_collection'] = test['belongs_to_collection'].apply(lambda x: len(x) if x != {} else 0)\n",
    "\n",
    "train = train.drop(['belongs_to_collection'], axis=1)\n",
    "test = test.drop(['belongs_to_collection'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b4dd196459d40795b0dddcb7274c38dff7b1562a",
    "colab_type": "text",
    "id": "vq8XTDDP7HIG"
   },
   "source": [
    "## **Genres**\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "87d4050a9e14f88f0b4274606c58fabfc1c00ada",
    "colab": {},
    "colab_type": "code",
    "id": "cex3yj-w7HIH"
   },
   "outputs": [],
   "source": [
    "train['num_genres'] = train['genres'].apply(lambda x: len(x) if x != {} else 0)\n",
    "train['all_genres'] = train['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\n",
    "\n",
    "test['num_genres'] = test['genres'].apply(lambda x: len(x) if x != {} else 0)\n",
    "test['all_genres'] = test['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NhAWJ_S87HIK"
   },
   "source": [
    "## **production_companies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NdrfW9Bl7HIL"
   },
   "outputs": [],
   "source": [
    "train['num_production_companies'] = train['production_companies'].apply(lambda x: len(x) if x != {} else 0)\n",
    "train['all_production_companies'] = train['production_companies'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\n",
    "\n",
    "test['num_production_companies'] = test['production_companies'].apply(lambda x: len(x) if x != {} else 0)\n",
    "test['all_production_companies'] = test['production_companies'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6VhyHpiB7HIQ"
   },
   "source": [
    "## **production_countries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zxQRu7Wr7HIR"
   },
   "outputs": [],
   "source": [
    "train['num_production_countries'] = train['production_countries'].apply(lambda x: len(x) if x != {} else 0)\n",
    "train['all_production_countries'] = train['production_countries'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\n",
    "\n",
    "test['num_production_countries'] = test['production_countries'].apply(lambda x: len(x) if x != {} else 0)\n",
    "test['all_production_countries'] = test['production_countries'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wdp_5OBQ7HIT"
   },
   "source": [
    "## **Cast**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cj6tA69_7HIV"
   },
   "outputs": [],
   "source": [
    "# list_of_cast_names = list(train['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\n",
    "train['num_cast'] = train['cast'].apply(lambda x: len(x) if x != {} else 0)\n",
    "train['all_cast'] = train['cast'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\n",
    "\n",
    "test['num_cast'] = test['cast'].apply(lambda x: len(x) if x != {} else 0)\n",
    "test['all_cast'] = test['cast'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6n3Uyh9y7HIZ"
   },
   "source": [
    "## **Crew**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yNbP1MRY7HIZ"
   },
   "outputs": [],
   "source": [
    "# list_of_crew_names = list(train['crew'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\n",
    "train['num_crew'] = train['crew'].apply(lambda x: len(x) if x != {} else 0)\n",
    "train['all_crew'] = train['crew'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\n",
    "\n",
    "test['num_crew'] = test['crew'].apply(lambda x: len(x) if x != {} else 0)\n",
    "test['all_crew'] = test['crew'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azO10VSl7HIc"
   },
   "source": [
    "## **spoken_languages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ji43yKNn7HIc"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'spoken_languages'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml_scratch\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'spoken_languages'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-ff9ea37b8903>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# list_of_spokenlanguage_names = list(train['spoken_languages'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'num_spoken_languages'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'spoken_languages'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'all_spoken_languages'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'spoken_languages'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'num_spoken_languages'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'spoken_languages'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml_scratch\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml_scratch\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2657\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'spoken_languages'"
     ]
    }
   ],
   "source": [
    "# list_of_spokenlanguage_names = list(train['spoken_languages'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\n",
    "train['num_spoken_languages'] = train['spoken_languages'].apply(lambda x: len(x) if x != {} else 0)\n",
    "train['all_spoken_languages'] = train['spoken_languages'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\n",
    "\n",
    "test['num_spoken_languages'] = test['spoken_languages'].apply(lambda x: len(x) if x != {} else 0)\n",
    "test['all_spoken_languages'] = test['spoken_languages'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J52GTpeq7HIf"
   },
   "source": [
    "## **keyword**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "44usDyOf7HIg"
   },
   "outputs": [],
   "source": [
    "# list_of_Keywords = list(train['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\n",
    "train['num_Keywords'] = train['Keywords'].apply(lambda x: len(x) if x != {} else 0)\n",
    "train['all_Keywords'] = train['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\n",
    "test['num_Keywords'] = test['Keywords'].apply(lambda x: len(x) if x != {} else 0)\n",
    "test['all_Keywords'] = test['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wYPWrpDa7HIi"
   },
   "source": [
    "## 2.1.3. **Missing data**\n",
    "<br>\n",
    "\n",
    "- 결측치가 존재하는 변수들은 NaN값 유무에 따른 이산화형 변수로 변경하여 분석에 이용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wYPWrpDa7HIi"
   },
   "source": [
    "## **homepage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P6XVFtM07HIj"
   },
   "outputs": [],
   "source": [
    "train['has_homepage'] = 1\n",
    "train.loc[pd.isnull(train['homepage']) ,\"has_homepage\"] = 0\n",
    "test['has_homepage'] = 1\n",
    "test.loc[pd.isnull(test['homepage']) ,\"has_homepage\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VnN2_bIM7HIl"
   },
   "source": [
    "## **tagline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZMHUqTt17HIl"
   },
   "outputs": [],
   "source": [
    "train['isTaglineNA'] = 0\n",
    "train.loc[pd.isnull(train['tagline']) ,\"isTaglineNA\"] = 1\n",
    "test['isTaglineNA'] = 0\n",
    "test.loc[pd.isnull(test['tagline']) ,\"isTaglineNA\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAzxK2eq7HIn"
   },
   "source": [
    "## **original, title**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "itf80I8Q7HIo"
   },
   "outputs": [],
   "source": [
    "train['isTitleDifferent'] = 1\n",
    "train.loc[ train['original_title'] == train['title'] ,\"isTitleDifferent\"] = 0\n",
    "test['isTitleDifferent'] = 1\n",
    "test.loc[ test['original_title'] == test['title'] ,\"isTitleDifferent\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **original_languages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['isOriginalLanguageEng'] = 0\n",
    "test['isOriginalLanguageEng'] = 0\n",
    "\n",
    "train.loc[train['original_language'] == \"en\" ,\"isOriginalLanguageEng\"] = 1\n",
    "test.loc[test['original_language'] == \"en\" ,\"isOriginalLanguageEng\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **spoken_languages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['isSpokenLanguageEng'] = 0\n",
    "train.loc[train['all_spoken_languages'] == \"English\" ,\"isSpokenLanguageEng\"] = 1\n",
    "test['isSpokenLanguageEng'] = 0\n",
    "test.loc[test['all_spoken_languages'] == \"English\" ,\"isSpokenLanguageEng\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_bOAgg757HIq"
   },
   "source": [
    "# **2.1.4. 연속형 변수 변환**\n",
    "- 연속형 변수들 중에서 치우친 분포를 가진 변수들은 log를 취하여 분석에 이용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gYekovWe7HIx"
   },
   "outputs": [],
   "source": [
    "# Putting log variables for skewed data \n",
    "train['log_budget']=np.log1p(train['budget'])\n",
    "test['log_budget']=np.log1p(test['budget'])\n",
    "\n",
    "train['log_runtime']=np.log1p(train['runtime'])\n",
    "test['log_runtime']=np.log1p(test['runtime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1tk_36D77HIz"
   },
   "source": [
    "# **3. Data modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AET33WFZ7HIz"
   },
   "outputs": [],
   "source": [
    "# Formating for modeling\n",
    "\n",
    "used_features = ['release_year', 'num_genres', 'all_genres',\n",
    "       'num_production_companies', 'all_production_companies',\n",
    "       'num_production_countries', 'all_production_countries', 'num_cast',\n",
    "       'all_cast', 'num_crew', 'all_crew', 'num_spoken_languages',\n",
    "       'all_spoken_languages', 'num_Keywords', 'all_Keywords', 'has_homepage',\n",
    "       'isTaglineNA', 'isTitleDifferent', 'log_budget', 'log_runtime', \n",
    "                 'isOriginalLanguageEng', 'isSpokenLanguageEng']\n",
    "\n",
    "X = train[used_features]\n",
    "y = np.log1p(train['revenue'])\n",
    " \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 761,
     "status": "error",
     "timestamp": 1556236518872,
     "user": {
      "displayName": "SSBB",
      "photoUrl": "",
      "userId": "02192221719466589460"
     },
     "user_tz": -540
    },
    "id": "f3zkYfu37HI1",
    "outputId": "23edbbbf-a218-4ac1-d146-d7f9622fccad"
   },
   "source": [
    "## **base model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 761,
     "status": "error",
     "timestamp": 1556236518872,
     "user": {
      "displayName": "SSBB",
      "photoUrl": "",
      "userId": "02192221719466589460"
     },
     "user_tz": -540
    },
    "id": "f3zkYfu37HI1",
    "outputId": "23edbbbf-a218-4ac1-d146-d7f9622fccad"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3739f89979f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlgb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBMRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m lgb_model.fit(X_train, y_train, \n\u001b[1;32m      3\u001b[0m         \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmse'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         verbose=1000, early_stopping_rounds=200)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lgb' is not defined"
     ]
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMRegressor(**params, n_estimators = 10000, nthread = 4, n_jobs = -1)\n",
    "lgb_model.fit(X_train, y_train, \n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric='rmse',\n",
    "        verbose=1000, early_stopping_rounds=200)\n",
    "\n",
    "print(\"훈련 점수: {:.2f}\".format(lgb_model.score(X_train, y_train)))\n",
    "print(\"테스트 점수: {:.2f}\".format(lgb_model.score(X_test, y_test)))\n",
    "\n",
    "eli5.show_weights(lgb_model, feature_filter=lambda x: x != '<BIAS>')\n",
    "\n",
    "y_pred = lgb_model.predict(test[used_features])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "project_basemodel.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
